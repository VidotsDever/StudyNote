`DataFrame`转化为`Dataset`：

```
val df: DataFrame = spark.read
      .schema(schema)
      .option("delimiter", "\t")
      .csv("dataset/studenttab10k")
val ds: Dataset[Student] = df.as[Student]

case class Student(name: String, age: Int, gpa: Int)
```



`DataFrame`中选择列：

```
ds.select('name).show()

ds.selectExpr("count(*)").show()

import org.apache.spark.sql.functions._
ds.select(expr("count(*)")).show()
```



`DataFrame`中新建列：

```
import org.apache.spark.sql.functions._

ds.withColumn("random", expr("rand()")).show() // 创建新列，使用随机数填充数据

ds.withColumn("name_new", 'name).show() // 创建新列，使用已存在的name列数据

ds.withColumnRenamed("name", "name_modified").show() // 修改name的列名
```



`DataFrame`剪除列：

```
ds.drop("age").show()
```



#### Column对象

创建列对象：

* `'`：单引号在Scala中是一个特殊的符号，通过其会生成一个`Symbol`对象，在Spark中，对`Symbol`对象做了隐式转换，转化为`ColumnName`对象，它是`Column`的子类。`df.select('name, 'age')`
* `$`：`$` 符号也是一个隐式转换，通过其可以生成一个`Column`对象。`df.select($"name", $"age")`
* `col`、`column`：SparkSQL提供一系列函数，其中有两个可以创建`Column`对象，它们是`col`与`column`，使用前必须导入`functions`包；
* `Dataset.col`：前面方式创建的`Column`对象都是Free的，也就是没有绑定任何`Dataset`；而该方法创建的对象是绑定到`Dataset上`的；
* `Dataset.apply`



#### 别名和转换

* `as[Type]`：将一个列中数据的类型转为`Type`类型：`col("age").as[Long]`
* `as(name)`：为列创建别名：`col("age").as("age_new")`



```
// 增加列
ds.withColumn("doubled", 'agev * 2)

// 模糊查询
ds.where('name like "wei%")

// 排序，正反序
ds.sort('age asc)

// 枚举判断
ds.where('name isin ("sunny", "ricky", "david"))
```



#### 缺失值处理

**丢失处理**：

```
# any，只要有一个NaN(Double下的异常情况)的行就丢弃
df.na.drop("any").show()

# all，所有数据都是NaN的行才丢弃
df.na.drop("all").show()

# 某些列的规则
df.na.drop("any", List("year", "month", "day", "hour")).show()
```



**填充处理：**

```
# 针对所有列数据进行默认值填充
df.na.fill(0).show()

# 针对特定列填充
df.na.fill(0, List("year", "month", "day")).show()
```

























