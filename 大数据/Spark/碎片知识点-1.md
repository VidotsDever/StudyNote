Spark是MapReduce的替代方案，并且兼容HDFS、Hive，可容入Hadoop的生态系统。

Spark提供了统一的解决方案，可以用于批处理、交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLib)和图计算(GraphX)。



Spark集群：分为`Master`和`Worker`，可以部署多台`Master`，Spark独立于其它存储系统，包括Hadoop。



Spark的主要数据结构：

* `DataSet`
* `DataFrame`：相当于`DataSet[Row]`
* `RDD`：Resilient Distributed Dataset

#### 搭建环境

下载到某个指定的目录，并解压。



进入到`conf`目录下，修改文件名：

```
mv spark-env.sh.template spark-env.sh

mv slaves.template slaves
```



在`spark-env.sh`文件中配置：

```
export JAVA_HOME=/home/vidots/Softwares/jdk
export SPARK_MASTER_HOST=hadoop
export SPARK_MASTER_PORT=7077
```

在`slaves`文件中配置Worker节点：

```
主机名或ip地址
```



启动集群：`sbin/start-all.sh`，Web页面：`Master机器ip地址:8080`



#### 提交Spark应用到集群中运行



`Master`负责资源调度，就是分配资源(就是决定`Executor`在哪些`Worker`上启动)

Spark安装目录中的`examples/jars/spark-examples_xx.jar`文件是官方提供的一个Spark应用，运行方法：进入其中一台`Worker`机器中，运行命令：

```
$ bin/spark-submit --master spark://master1:7077[,master2:7077] --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.4.6.jar  100

```

提交Spark程序到Spark集群，会产生的进程：

* `SparkSubmit`(`Driver`) 提交任务；
* `Executor` 执行真正的任务；



Spark Shell是一个交互式的命令行，也是一个客户端，用于提交Spark应用程序。启动

* `$ bin/spark-shell`：没有指定master的地址，用的是Spark的local模式运行的；
* `$ bin/spark-shell --master spark://hadoop:7077`



用Spark Shell完成WordCount计算：

启动HDFS，`sc`是Spark Core的执行入口

```
sc.textFile("hdfs://hadoop:9000/wordcount/input").flatMap(_.split("")).map((_, 1)).reduceByKey(_ + _).collect
```



Spark的`StandAlone`调度模式：不依赖其它调度框架；



